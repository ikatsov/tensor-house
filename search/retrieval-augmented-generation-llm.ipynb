{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cf6cc2-b3cf-4843-b502-71872d3dde35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Retrieval-augmented Generation (RAG) Using LLMs \n",
    "\n",
    "This notebook provides several prototypes of a Retrieval-Augmented Generation (RAG) system. \n",
    "\n",
    "### Use Case\n",
    "We have a large collection of documents and want to use LLM to summarize these documents, answer standalone questions based on the document content, or answer questions in a conversational mode. Examples include a sales assistant that answers customers' questions about company's products, coding assistant that answers developers' questions about the codebase, and legal assistant that answers questions about regulations.   \n",
    "\n",
    "### Prototype: Approach and Data\n",
    "We start with a basic case where the input documents are small enough to fit the LLM context, and then develop more advanced solutions that can handle large document collections. We use small input documents that are available in the `tensor-house-data` repository.\n",
    "\n",
    "### Usage and Productization\n",
    "The implementation uses a production-grade framework, but external embedding storage (vector store) and additional components such as caching are typically needed to create production grade applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0804d-029e-416d-8014-c7828d96e4d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Environment Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a97eb6a-ccec-4527-b9fb-36b69c6ff3de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Initialize LLM provider\n",
    "# (google-cloud-aiplatform must be installed)\n",
    "#\n",
    "from google.cloud import aiplatform\n",
    "aiplatform.init(\n",
    "    project='<< specify your project name here >>',\n",
    "    location='us-central1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a239de43-7783-46f3-b919-5ad39b0a2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Imports\n",
    "#\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.vertexai import VertexAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28f863-c2e2-4c56-b459-df05ff12ad8f",
   "metadata": {},
   "source": [
    "## Question Answering Using a Single Prompt\n",
    "\n",
    "The most basic scenario is querying small documents that fit the LLM prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf134242-352d-4281-8f57-099ca387143b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text states that \"Lemons and limes have particularly high concentrations of the acid; it can constitute as much as 8% of the dry weight of these fruits (about 47 g/L in the juices)\". So the correct answer is lemons and limes.\n"
     ]
    }
   ],
   "source": [
    "query_prompt = \"\"\"\n",
    "Please read the following text and answer which fruits have the highest concentration of citric acid.\n",
    "\n",
    "TEXT:\n",
    "Citric acid occurs in a variety of fruits and vegetables, most notably citrus fruits. Lemons and limes have particularly \n",
    "high concentrations of the acid; it can constitute as much as 8% of the dry weight of these fruits (about 47 g/L in the juices).\n",
    "The concentrations of citric acid in citrus fruits range from 0.005 mol/L for oranges and grapefruits to 0.30 mol/L in lemons \n",
    "and limes; these values vary within species depending upon the cultivar and the circumstances under which the fruit was grown.\n",
    "\"\"\"\n",
    "\n",
    "llm = VertexAI(temperature=0.7)\n",
    "response = llm(query_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869bf50-e4f2-4ba6-b7b9-2278c1a072a0",
   "metadata": {},
   "source": [
    "## Question Answering Using MapReduce\n",
    "\n",
    "For large documents and collections of documents that do not fit the LLM context, we can apply the MapReduce pattern to independently extract relevant summaries from document parts, and then merge these summaries into the final answer. This approach is appropriate for summarization and summarization-like queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5a43885-5f2e-49d3-b5cf-bc6204b10d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input document has been split into 2 chunks\n",
      "\n",
      "The three most important applications of citric acid are:\n",
      "1. As a flavoring and preservative in food and beverages, especially soft drinks.\n",
      "2. In soaps and laundry detergents to remove hard water stains.\n",
      "3. In the biotechnology and pharmaceutical industry to passivate high purity process piping.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load the input document\n",
    "#\n",
    "loader = TextLoader(\"../../tensor-house-data/search/food-additives/citric-acid-applications.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "#\n",
    "# Splitting\n",
    "#\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f'The input document has been split into {len(texts)} chunks\\n')\n",
    "\n",
    "#\n",
    "# Querying\n",
    "#\n",
    "question_prompt = \"\"\"\n",
    "Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
    "Return bullet points that help to answer the question.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Bullet points:\n",
    "\"\"\"\n",
    "question_prompt_template = PromptTemplate(template=question_prompt, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "combine_prompt = \"\"\"\n",
    "Given the following bullet points extracted from a long document and a question, create a final answer.\n",
    "Question: {question}\n",
    "\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "\n",
    "Final answer:\n",
    "\"\"\"\n",
    "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "llm = VertexAI(temperature=0.7)\n",
    "qa_chain = load_qa_chain(llm=llm,\n",
    "                         chain_type='map_reduce',\n",
    "                         question_prompt=question_prompt_template,\n",
    "                         combine_prompt=combine_prompt_template,\n",
    "                         verbose=False)\n",
    "\n",
    "question = \"What are the three most important applications of citric acid? Provide a short justification for each application.\"\n",
    "response = qa_chain({\"input_documents\": texts, \"question\": question})\n",
    "\n",
    "print(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb6e94-a2f2-4548-8d0c-f4934f0224f6",
   "metadata": {},
   "source": [
    "## Question Answering Using Vector Search\n",
    "\n",
    "For large documents and point questions that require only specific document parts to be answered, LLMs can be combined with traditional information retrieval techniques. The input document(s) is split into chunks which are then indexed in a vector store. To answer the user question, the most relevant chunks are retrieved and passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3857ccc4-2559-493d-9e44-babb744576db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input document has been split into 5 chunks\n",
      "\n",
      "The melting point of citric acid is approximately 153 °C (307 °F).\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load the input document\n",
    "#\n",
    "loader = TextLoader(\"../../tensor-house-data/search/food-additives/food-additives.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "#\n",
    "# Splitting\n",
    "#\n",
    "text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f'The input document has been split into {len(texts)} chunks\\n')\n",
    "\n",
    "#\n",
    "# Indexing and storing\n",
    "#\n",
    "embeddings = VertexAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "#\n",
    "# Querying\n",
    "#\n",
    "llm = VertexAI(temperature=0.7, verbose=True)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                       chain_type=\"stuff\", \n",
    "                                       retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}), \n",
    "                                       return_source_documents=True)\n",
    "\n",
    "question = \"What is the melting point of citric acid?\"\n",
    "response = qa_chain({\"query\": question})\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1eb2aa-9c11-429d-94eb-a718d44a3979",
   "metadata": {},
   "source": [
    "## Conversational Retrieval\n",
    "\n",
    "In this section, we prototype a conversational retrieval system. It combines the chat history with the retrieved documents to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "141daeb7-a343-4ac8-af55-fda046326a60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input document has been split into 5 chunks\n",
      "\n",
      "How much times aspartam is sweeter than table sugar? \n",
      "\n",
      "Aspartame is approximately 150 to 200 times sweeter than sucrose (table sugar), making it an intense sweetener. \n",
      "\n",
      "What is its caloric value? \n",
      "\n",
      "Aspartame is virtually calorie-free, as the human body does not metabolize it into energy like sugars or carbohydrates. \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load the input document\n",
    "#\n",
    "loader = TextLoader(\"../../tensor-house-data/search/food-additives/food-additives.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "#\n",
    "# Splitting\n",
    "#\n",
    "text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f'The input document has been split into {len(texts)} chunks\\n')\n",
    "\n",
    "#\n",
    "# Indexing and storing\n",
    "#\n",
    "embeddings = VertexAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "#\n",
    "# Initialize new chat\n",
    "#\n",
    "llm = VertexAI(verbose=True)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chat = ConversationalRetrievalChain.from_llm(llm, \n",
    "                                             retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}), \n",
    "                                             memory=memory, \n",
    "                                             verbose=False, \n",
    "                                             return_generated_question=False)\n",
    "\n",
    "#\n",
    "# Ask questions with a continuous context\n",
    "#\n",
    "def print_last_chat_turn(chat_history):\n",
    "    print(chat_history[-2].content, '\\n')\n",
    "    print(chat_history[-1].content, '\\n')    \n",
    "\n",
    "result = chat({\"question\": \"How much times aspartam is sweeter than table sugar?\"})\n",
    "print_last_chat_turn(result['chat_history'])\n",
    "\n",
    "result = chat({\"question\": \"What is its caloric value?\"})\n",
    "print_last_chat_turn(result['chat_history'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
