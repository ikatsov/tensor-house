{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Virtual Focus Groups Using LLMs\n",
    "\n",
    "This notebook demonstrates how to analyze user-generated content (UGC) using LLMs.\n",
    "\n",
    "#### Use Case\n",
    "We have large amounts of user-generate content such as product reviews or social media posts. We want to understand what users think about our brand, products, marketing communications, corporate social media posts, etc. Performing this analysis manually is a daunting labor-consuming task, but we can leverage language models to perform content summarization. \n",
    "\n",
    "One particular way of implementing this idea is to cluster the available content in the embedding space and create a *virtual persona* (or *virtual focus group*) for each cluster that impersonates the real users and answers questions based on the psychological characteristics and traits such as values, desires, goals, interests, and lifestyle choices expressed in the content. Marketing users can perform the analysis by asking questions to such virtual personas. This simulates the analysis using real focus groups, and can also be viewed as a cost- and time-efficient alternative to using real focus groups.\n",
    "\n",
    "#### Prototype: Data and Limitations \n",
    "We use the Amazon Product Review 2018 dataset (see `datasets.md` for details) for the prototyping purposes. The prototype uses a small subset of reviews that fit the LLM context.  \n",
    "\n",
    "#### Usage and Productization\n",
    "The input dataset can be easily replaced with actual reviews or social media posts; small data schema adjustments will be required. However, the basic prompt-based summarizations will need to be replaced with a more scalable approach such as retrieval-augmented generation (RAG). In practice, the personas would be typically specified by marketing users rather than extracted from content on an ad-hoc basis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "143c67214b039270"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#\n",
    "# Imports and helper functions\n",
    "#\n",
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from sklearn.cluster import KMeans\n",
    "import inspect\n",
    "\n",
    "def trim_multiline(x):\n",
    "    return inspect.cleandoc(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:28.791147Z",
     "start_time": "2023-12-10T14:37:27.439978Z"
    }
   },
   "id": "d10f1fe5eb1ef8a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the Review Data\n",
    "\n",
    "In this section, we load the product and review data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e4c99ce1f50c12e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 574628 reviews\n",
      "Loaded 12299 products\n"
     ]
    }
   ],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  df = {}\n",
    "  for i, d in enumerate(parse(path)):\n",
    "    df[i] = d\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "base_path = '<<path to data>>' # Replace with the path to the Amazon Review Data folder\n",
    "review_df = getDF(base_path + 'Luxury_Beauty.json.gz')\n",
    "print(f\"Loaded {len(review_df)} reviews\")\n",
    "\n",
    "meta_df = getDF(base_path + 'meta_Luxury_Beauty.json.gz')\n",
    "print(f\"Loaded {len(meta_df)} products\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:42.121191Z",
     "start_time": "2023-12-10T14:37:30.908694Z"
    }
   },
   "id": "e746762cf0509275"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Personas By Clustering Review Embeddings\n",
    "\n",
    "In this section, we compute embeddings for individual reviews, cluster the reviews in the embedding space to create the data foundations for virtual personas, and then identify the personas by analyzing the content in each cluster. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "717c51e8a54d4593"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following embedding model: project=None location='us-central1' request_parallelism=5 max_retries=6 stop=None model_name='textembedding-gecko' client=<vertexai.language_models.TextEmbeddingModel object at 0x7f92ede23880> client_preview=None temperature=0.0 max_output_tokens=128 top_p=0.95 top_k=40 credentials=None n=1 streaming=False\n",
      "Using the following generative model: \u001B[1mVertexAI\u001B[0m\n",
      "Params: {'model_name': 'text-bison', 'temperature': 0.7, 'max_output_tokens': 128, 'candidate_count': 1, 'top_k': 40, 'top_p': 0.95}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Initialize LLM provider\n",
    "# (google-cloud-aiplatform must be installed)\n",
    "#\n",
    "from google.cloud import aiplatform\n",
    "aiplatform.init(\n",
    "    project='gd-gcp-rnd-genai-convai',\n",
    "    location='us-central1'\n",
    ")\n",
    "embedding_llm = VertexAIEmbeddings()\n",
    "print(f'Using the following embedding model: {embedding_llm}')\n",
    "\n",
    "llm = VertexAI(temperature=0.7)\n",
    "print(f'Using the following generative model: {llm}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T15:22:11.174482Z",
     "start_time": "2023-12-10T15:22:09.898262Z"
    }
   },
   "id": "2643c5fc3d043ad1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 4733 reviews for 173 products.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Sample and filter the reviews \n",
    "#\n",
    "n_products = 200\n",
    "meta_sample_df = meta_df.sample(n_products)\n",
    "df = pd.merge(meta_sample_df[['asin', 'title']], \n",
    "              review_df[['asin', 'reviewerName', 'summary', 'reviewText']], \n",
    "              how='inner', on='asin')\n",
    "\n",
    "df[['summary', 'reviewText']] = df[['summary', 'reviewText']].astype(str)\n",
    "df = df[(df['summary'].map(len) > 10) & (df['reviewText'].map(len) > 100)]   # Filter out too short reviews\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Sampled {df.shape[0]} reviews for {df['asin'].nunique()} products\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:40:35.230379Z",
     "start_time": "2023-12-10T14:40:34.942218Z"
    }
   },
   "id": "9afca0ea3cf518a5"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 4733 embedding vectors\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Compute embeddings for the reviews\n",
    "#\n",
    "def row_to_document(row):\n",
    "    return trim_multiline(f\"\"\"\n",
    "    Product title: {row[\"title\"]}\n",
    "    Review summary: {row[\"summary\"]}\n",
    "    Review text: {row[\"reviewText\"]}\"\"\")\n",
    "\n",
    "docs = df.apply(lambda review: row_to_document(review), axis=1).to_list()\n",
    "\n",
    "embeddings = embedding_llm.embed_documents(docs)\n",
    "print(f'Computed {len(embeddings)} embedding vectors')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:52:52.249313Z",
     "start_time": "2023-12-10T14:49:49.835910Z"
    }
   },
   "id": "a27897ea86f1242e"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 4 personas have been identified:\n",
      "{0: ' The Sophisticated Beauty Connoisseur',\n",
      " 1: ' The Versatile Hair Enthusiast',\n",
      " 2: ' The Refined Grooming Enthusiast',\n",
      " 3: ' The Skincare Enthusiast'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Perform clustering and attribute each review with the cluster ID\n",
    "#\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\").fit(embeddings)\n",
    "clustered_df = pd.concat([df, pd.DataFrame(kmeans.labels_, columns=['cluster_id'])], axis=1)\n",
    "\n",
    "#\n",
    "# Analyze each cluster and create a persona title\n",
    "#\n",
    "topic_prompt = \"\"\"You're an expert marketing analyst. Your goal is to read of bunch of product reviews and create a title for an imaginary persona who wrote these reviews. \n",
    "The title should clearly describe the psychological characteristics and traits such as values, desires, goals, interests, and lifestyle choices. \n",
    "The title MUST NOT contain any formatting or personal names. \n",
    "\n",
    "---------------------------\n",
    "PRODUCT REVIEWS:\n",
    "{reviews}\n",
    "---------------------------\n",
    "\n",
    "\n",
    "\n",
    "PERSONA TITLE:\n",
    "\"\"\"\n",
    "topic_prompt_template = PromptTemplate(template=topic_prompt, input_variables=[\"reviews\"])\n",
    "\n",
    "n_samples_per_cluster = 50\n",
    "sampled_reviews_df = clustered_df.groupby('cluster_id').sample(n=n_samples_per_cluster)\n",
    "personas = {}\n",
    "for cluster_id in range(n_clusters):\n",
    "    reviews_in_cluster = sampled_reviews_df[sampled_reviews_df['cluster_id']==cluster_id].apply(\n",
    "        lambda review: row_to_document(review), axis=1).to_list()\n",
    "\n",
    "    response = llm(topic_prompt_template.format(reviews=\"\\n\\n\\n\".join(reviews_in_cluster)))\n",
    "    personas[cluster_id] = response\n",
    "\n",
    "print(f'The following {len(personas)} personas have been identified:')\n",
    "pprint(personas)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T15:24:46.514183Z",
     "start_time": "2023-12-10T15:24:38.423768Z"
    }
   },
   "id": "7fbe713c876e2cf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ask Questions to Virtual Personas\n",
    "\n",
    "In this section, we demonstrate how business users (e.g. marketing analysts) can interact with the virtual personas and get insights into customers' perception of the brand, product, and messaging."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de67594456a7175b"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To improve the products, the following recommendations can be made:\n",
      "\n",
      "1. **Improve the quality of the brush in the Billy Jealousy Beard Envy Kit.** Many reviewers complained that the brush was too stiff and caused irritation. A softer brush would be more comfortable to use and would not damage the beard.\n",
      "\n",
      "2. **Make the conditioner in the Billy Jealousy Beard Envy Kit more moisturizing.** Some reviewers felt that the conditioner did not provide enough moisture. A more moisturizing conditioner would help to keep the beard soft and healthy.\n",
      "\n",
      "3. **Improve the packaging of the Art of Shaving Full Size Kit.** Some reviewers felt that the packaging was\n"
     ]
    }
   ],
   "source": [
    "persona_prompt = \"\"\"You're a virtual focus group persona defined as {persona_title}. Please answer the question consistently with your previous product reviews. \n",
    "Your answer must be consistent with your reviews in terms of values, desires, goals, interests, lifestyle choices, and reported product strengths and weaknesses.\n",
    "\n",
    "YOUR PREVIOUS REVIEWS:\n",
    "{reviews}\n",
    "\n",
    "---------------------------\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "persona_prompt_template = PromptTemplate(template=persona_prompt, input_variables=[\"persona_title\", \"reviews\", \"question\"])\n",
    "\n",
    "cluster_id = 2\n",
    "persona_reviews = sampled_reviews_df[sampled_reviews_df['cluster_id']==cluster_id].apply(\n",
    "        lambda review: row_to_document(review), axis=1)\n",
    "person_review_as_text = \"\\n\\n\".join(persona_reviews)\n",
    "\n",
    "question = \"How you would recommend to improve the products?\"\n",
    "\n",
    "response = llm(persona_prompt_template.format(persona_title=personas[cluster_id], reviews=person_review_as_text, question=question))\n",
    "\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T15:24:52.162445Z",
     "start_time": "2023-12-10T15:24:48.439845Z"
    }
   },
   "id": "1eea98a2b9dd7b3a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "py310",
   "language": "python",
   "display_name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
