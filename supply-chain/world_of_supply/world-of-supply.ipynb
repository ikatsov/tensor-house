{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World of Supply\n",
    "\n",
    "A simulation environment for multi-echelon supply chain optimization problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm\n",
    "import importlib\n",
    "\n",
    "import world_of_supply_tools as wst\n",
    "importlib.reload(wst)\n",
    "\n",
    "wst.print_hardware_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Simulation Logic and Rendering\n",
    "\n",
    "In this section, we test the core simulator and renderer (without RL adapters and integrations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import world_of_supply_environment as ws\n",
    "import world_of_supply_renderer as wsr\n",
    "import world_of_supply_tools as wst\n",
    "for module in [ws, wsr, wst]:\n",
    "    importlib.reload(module)\n",
    "        \n",
    "# Measure the simulation rate, steps/sec\n",
    "eposod_len = 1000\n",
    "n_episods = 10\n",
    "world = ws.WorldBuilder.create()\n",
    "tracker = wst.SimulationTracker(eposod_len, n_episods, world.facilities.keys())\n",
    "with tqdm(total=eposod_len * n_episods) as pbar:\n",
    "    for i in range(n_episods):\n",
    "        world = ws.WorldBuilder.create()\n",
    "        policy = ws.SimpleControlPolicy()\n",
    "        for t in range(eposod_len):\n",
    "            outcome = world.act(policy.compute_control(world))\n",
    "            tracker.add_sample(i, t, world.economy.global_balance().total(), \n",
    "                               {k: v.total() for k, v in outcome.facility_step_balance_sheets.items() } )\n",
    "            pbar.update(1)        \n",
    "tracker.render()\n",
    "    \n",
    "# Test rendering\n",
    "renderer = wsr.AsciiWorldRenderer()\n",
    "frame_seq = []\n",
    "world = ws.WorldBuilder.create()\n",
    "policy = ws.SimpleControlPolicy()\n",
    "for epoch in tqdm(range(300)):\n",
    "    frame = renderer.render(world)\n",
    "    frame_seq.append(np.asarray(frame))\n",
    "    world.act(policy.compute_control(world))\n",
    "\n",
    "print('Rendering the animation...')\n",
    "wsr.AsciiWorldRenderer.plot_sequence_images(frame_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Training\n",
    "\n",
    "In this section, we run RLlib policy trainers. These trainers evaluate the hand coded policy, learn a new policy from scrath, or learn a new policy by playing against the hand coded policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import world_of_supply_rllib_models as wsm\n",
    "importlib.reload(wsm)\n",
    "import world_of_supply_rllib as wsrl\n",
    "importlib.reload(wsrl)\n",
    "import world_of_supply_rllib_training as wsrt\n",
    "importlib.reload(wsrt)\n",
    "\n",
    "wsrt.print_model_summaries()\n",
    "\n",
    "# Policy training\n",
    "#trainer = wsrt.play_baseline(n_iterations = 1)\n",
    "trainer = wsrt.train_ppo(n_iterations = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "In this section, we evaluate the trained policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering One Episod for the Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import world_of_supply_renderer as wsren\n",
    "import world_of_supply_tools as wst\n",
    "import world_of_supply_rllib as wsrl\n",
    "import world_of_supply_rllib_training as wstr\n",
    "for module in [wsren, wst, wsrl, wstr]:\n",
    "    importlib.reload(module)\n",
    "\n",
    "# Parameters of the tracing simulation\n",
    "policy_mode = 'baseline'   # 'baseline' or 'trained'\n",
    "episod_duration = 500\n",
    "steps_to_render = None#(0, episod_duration)  # (0, episod_duration) or None\n",
    "\n",
    "# Create the environment\n",
    "renderer = wsren.AsciiWorldRenderer()\n",
    "frame_seq = []\n",
    "env_config_for_rendering = wstr.env_config.copy()\n",
    "env_config_for_rendering.update({\n",
    "    'downsampling_rate': 1\n",
    "})\n",
    "env = wsrl.WorldOfSupplyEnv(env_config_for_rendering)\n",
    "env.set_iteration(1, 1)\n",
    "print(f\"Environment: Producer action space {env.action_space_producer}, Consumer action space {env.action_space_consumer}, Observation space {env.observation_space}\")\n",
    "states = env.reset()\n",
    "infos = None\n",
    "    \n",
    "def load_policy(agent_id):\n",
    "    if policy_mode == 'baseline':\n",
    "        if wsrl.Utils.is_producer_agent(agent_id):\n",
    "            return wsrl.ProducerSimplePolicy(env.observation_space, env.action_space_producer, wsrl.SimplePolicy.get_config_from_env(env))\n",
    "        elif wsrl.Utils.is_consumer_agent(agent_id):\n",
    "            return wsrl.ConsumerSimplePolicy(env.observation_space, env.action_space_consumer, wsrl.SimplePolicy.get_config_from_env(env))\n",
    "        else:\n",
    "            raise Exception(f'Unknown agent type {agent_id}')\n",
    "    \n",
    "    if policy_mode == 'trained':\n",
    "        policy_map = wstr.policy_mapping_global.copy()\n",
    "        wstr.update_policy_map(policy_map)   \n",
    "        return trainer.get_policy(wstr.create_policy_mapping_fn(policy_map)(agent_id))\n",
    "\n",
    "policies = {}\n",
    "rnn_states = {}\n",
    "for agent_id in states.keys():\n",
    "    policies[agent_id] = load_policy(agent_id)\n",
    "    rnn_states[agent_id] = policies[agent_id].get_initial_state()\n",
    "    \n",
    "# Simulation loop\n",
    "tracker = wst.SimulationTracker(episod_duration, 1, env.agent_ids())\n",
    "for epoch in tqdm(range(episod_duration)):\n",
    "    \n",
    "    action_dict = {}\n",
    "    if epoch % wstr.env_config['downsampling_rate'] == 0:\n",
    "        for agent_id, state in states.items():\n",
    "            policy = policies[agent_id]\n",
    "            rnn_state = rnn_states[agent_id]\n",
    "            if infos is not None and agent_id in infos:\n",
    "                action_dict[agent_id], rnn_state, _ = policy.compute_single_action( state, info=infos[agent_id], state=rnn_state ) \n",
    "            else:\n",
    "                action_dict[agent_id], rnn_state, _ = policy.compute_single_action( state, state=rnn_state )   \n",
    "   \n",
    "    states, rewards, dones, infos = env.step(action_dict)\n",
    "    tracker.add_sample(0, epoch, env.world.economy.global_balance().total(), rewards)\n",
    "    \n",
    "    if steps_to_render is not None and epoch >= steps_to_render[0] and epoch < steps_to_render[1]:\n",
    "        frame = renderer.render(env.world)\n",
    "        frame_seq.append(np.asarray(frame))\n",
    " \n",
    "tracker.render()\n",
    "\n",
    "if steps_to_render is not None:\n",
    "    print('Rendering the animation...')\n",
    "    wsren.AsciiWorldRenderer.plot_sequence_images(frame_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
